{"cells":[{"cell_type":"markdown","metadata":{"id":"UUnI43xyYm8E"},"source":["# Mounting Content from Google Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ExN8zjkkMfWQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639777258412,"user_tz":-120,"elapsed":13851,"user":{"displayName":"Nick","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09933765762641469766"}},"outputId":"d7d1cefa-bb7f-4541-9ed1-9aa73fb9f390"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"]},{"cell_type":"markdown","metadata":{"id":"jI9Q9G7nYgok"},"source":["# Import necessary libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ab_qYKM7iGGS"},"outputs":[],"source":["%matplotlib inline\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns"]},{"cell_type":"markdown","metadata":{"id":"se1EI84eaQGK"},"source":["# Import datasets\n"]},{"cell_type":"markdown","metadata":{"id":"gVfGUJ2Waa-b"},"source":["Labels in dataset:\n","*   label 0: neutral\n","*   label 1: anti-vax\n","*   label 2: pro-vax\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":399},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1639781071167,"user":{"displayName":"Nick","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09933765762641469766"},"user_tz":-120},"id":"JEDnaZCdxXHI","outputId":"a3240d6e-0932-44c7-b755-5baf1a0138e4"},"outputs":[{"output_type":"stream","name":"stdout","text":["vaccine_validation_set: (15976, 2)\n","vaccine_train_set: (2282, 2)\n"]},{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-d25d81bf-6f49-4da9-b9c7-5a13916fd2ff\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tweet</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Sip N Shop Come thru right now #Marjais #Popul...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>I don't know about you but My family and I wil...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>@MSignorile Immunizations should be mandatory....</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>President Obama spoke in favor of vaccination ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>\"@myfoxla: Arizona monitoring hundreds for mea...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Why did I get my whooping cough vaccine the sa...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Prosecutor Ken Ervin: I want to know how reckl...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>\"@UberFacts: On average, people who complain l...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>The legacy of @JennyMcCarthy will be she took ...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>“@UberFacts: On average, people who complain l...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d25d81bf-6f49-4da9-b9c7-5a13916fd2ff')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-d25d81bf-6f49-4da9-b9c7-5a13916fd2ff button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-d25d81bf-6f49-4da9-b9c7-5a13916fd2ff');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                                               tweet  label\n","0  Sip N Shop Come thru right now #Marjais #Popul...      0\n","1  I don't know about you but My family and I wil...      1\n","2  @MSignorile Immunizations should be mandatory....      2\n","3  President Obama spoke in favor of vaccination ...      0\n","4  \"@myfoxla: Arizona monitoring hundreds for mea...      0\n","5  Why did I get my whooping cough vaccine the sa...      2\n","6  Prosecutor Ken Ervin: I want to know how reckl...      0\n","7  \"@UberFacts: On average, people who complain l...      0\n","8  The legacy of @JennyMcCarthy will be she took ...      2\n","9  “@UberFacts: On average, people who complain l...      0"]},"metadata":{},"execution_count":11}],"source":["# train set\n","Train_set_Location = r'/content/vaccine_train_set.csv' \n","\n","# validation set\n","Validation_set_Location = r'/content/vaccine_validation_set.csv'  \n","\n","df_train = pd.read_csv(Train_set_Location, index_col=0)\n","df_test = pd.read_csv(Validation_set_Location, index_col=0)\n","\n","print(f\"vaccine_validation_set: {df_train.shape}\")\n","print(f\"vaccine_train_set: {df_test.shape}\")\n","\n","df_train.head(10) # check the train dataframe"]},{"cell_type":"markdown","metadata":{"id":"bs4kZchyYaZj"},"source":["# Clean up the data\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":273,"status":"ok","timestamp":1639781073491,"user":{"displayName":"Nick","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09933765762641469766"},"user_tz":-120},"id":"pq7MYMbIGOcn","outputId":"d35434a3-2aaf-4591-8542-e400f5543866"},"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-7bea1e51-f758-436b-b90a-28b266062ea1\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tweet</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Sip N Shop Come thru right now #Marjais #Popul...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>I don't know about you but My family and I wil...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>@MSignorile Immunizations should be mandatory....</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>President Obama spoke in favor of vaccination ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>\"@myfoxla: Arizona monitoring hundreds for mea...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7bea1e51-f758-436b-b90a-28b266062ea1')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-7bea1e51-f758-436b-b90a-28b266062ea1 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-7bea1e51-f758-436b-b90a-28b266062ea1');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                                               tweet  label\n","0  Sip N Shop Come thru right now #Marjais #Popul...      0\n","1  I don't know about you but My family and I wil...      1\n","2  @MSignorile Immunizations should be mandatory....      2\n","3  President Obama spoke in favor of vaccination ...      0\n","4  \"@myfoxla: Arizona monitoring hundreds for mea...      0"]},"metadata":{},"execution_count":12}],"source":["# Drop the rows where at least one element is missing\n","df_train.dropna()\n","df_test.dropna()\n","\n","df_train.head(5)"]},{"cell_type":"markdown","metadata":{"id":"Uh3nki1N1zlr"},"source":["# Creating GloVe vocabulary "]},{"cell_type":"markdown","metadata":{"id":"-Mdq7KoknsTd"},"source":["##Download pre-trained model\n","https://nlp.stanford.edu/projects/glove/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":348919,"status":"ok","timestamp":1639777961669,"user":{"displayName":"Nick","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09933765762641469766"},"user_tz":-120},"id":"54iqkRnejYZN","outputId":"854a9b43-3659-450e-be62-018c6c9e0365"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2021-12-17 21:46:53--  https://nlp.stanford.edu/data/glove.twitter.27B.zip\n","Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: http://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip [following]\n","--2021-12-17 21:46:53--  http://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip\n","Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n","Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1520408563 (1.4G) [application/zip]\n","Saving to: ‘glove.twitter.27B.zip’\n","\n","glove.twitter.27B.z 100%[===================>]   1.42G  5.08MB/s    in 4m 46s  \n","\n","2021-12-17 21:51:40 (5.07 MB/s) - ‘glove.twitter.27B.zip’ saved [1520408563/1520408563]\n","\n","Archive:  glove.twitter.27B.zip\n","  inflating: glove.twitter.27B.25d.txt  \n","  inflating: glove.twitter.27B.50d.txt  \n","  inflating: glove.twitter.27B.100d.txt  \n","  inflating: glove.twitter.27B.200d.txt  \n"]}],"source":["!wget https://nlp.stanford.edu/data/glove.twitter.27B.zip\n","!unzip glove.twitter.27B.zip"]},{"cell_type":"markdown","metadata":{"id":"lzYy6kQFOsB_"},"source":["## Import glove.twitter file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1EpeuFKeO6-n"},"outputs":[],"source":["# Using the vocabulary with 25 dimensions\n","glove_twitter_Location = '/content/glove.twitter.27B.25d.txt' # change this if you want\n","D = int(glove_twitter_Location.split('.')[3][:-1])            # get the dimensions from the vectors in file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8D1Pr_9lqTUO"},"outputs":[],"source":["# Getting the vocabulary from the file\n","f = open(glove_twitter_Location, \"r\") \n","\n","glove_words = dict()   # key : word, value : index to vector from gloves_vectors\n","glove_vectors_list = []\n","index_vector = 0\n","\n","# for each line in glove file get the word in a dict and the vector in an list\n","for  x in f:  \n","  word = x.split()[0]\n","  vector = [ float(n) for n in x.split()[1:] ]  # convert each string to float\n","  if len(vector) != D :              # pass the Unreadable characters\n","    continue\n","  glove_words[word] = index_vector   # so a word has the vector at index -> index_vector to gloves_vectors\n","  glove_vectors_list.append(vector)    \n","  index_vector = index_vector + 1\n","gloves_vectors = np.asarray(glove_vectors_list)  # convert list to array\n","\n","f.close()"]},{"cell_type":"markdown","metadata":{"id":"ZJTa2-p-DsS9"},"source":["## Convert tweets to vectors "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3463,"status":"ok","timestamp":1639781124498,"user":{"displayName":"Nick","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09933765762641469766"},"user_tz":-120},"id":"xX8UkLTtFY02","outputId":"26c48304-9c71-4849-d5c6-9dc80439651d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["15976"]},"metadata":{},"execution_count":15}],"source":["# for train set build a dictionary of words and an array of vectors for each tweet\n","all_vectors_train = []  \n","\n","n_words = 0\n","\n","for index, row in df_train.iterrows():\n","  n_words = 0\n","  prev_vector = np.array([0]*D)   # create a null list\n","\n","  for word in row['tweet'].split() :\n","    n_words += 1\n","    if word in glove_words:             # for each word in tweet get the vector from glove_vectors, if vector doesn't exist pass\n","      index_vector = glove_words[word]  \n","      curr_word_vector = np.array(glove_vectors_list[index_vector])\n","      tweet_vectors = curr_word_vector + prev_vector  # Add all vectors in tweet, so at the end we will have a vector with D dimensions for each tweet\n","      prev_vector = tweet_vectors\n","\n","  all_vectors_train.append(tweet_vectors/n_words) # divide with the sum of all elements to normalize values\n","\n","all_vectors_train = np.array(all_vectors_train)\n","\n","len(all_vectors_train)  # should be equal with the number of tweets from train set"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":702,"status":"ok","timestamp":1639781126054,"user":{"displayName":"Nick","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09933765762641469766"},"user_tz":-120},"id":"ZSnO5SQBVkgy","outputId":"7dd8cb93-000f-4dbe-f318-e17392cc0eae"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["2282"]},"metadata":{},"execution_count":16}],"source":["# Same for validation set\n","all_vectors_test = []  \n","  \n","n_words = 0\n","\n","for index, row in df_test.iterrows():\n","  n_words = 0\n","  prev_vector = np.array([0]*D)   # create a null list\n","\n","  for word in row['tweet'].split() :\n","    n_words += 1\n","    if word in glove_words:             # for each word in tweet get the array from glove_vectors\n","      index_vector = glove_words[word]  # get the vector for the word\n","      curr_word_vector = np.array(glove_vectors_list[index_vector])\n","      tweet_vectors = curr_word_vector + prev_vector    # Add all vectors in tweet, so at the end we will have a vector with D dimensions for each tweet\n","      prev_vector = tweet_vectors\n","    # else:       # if word doesn't exist in glove vocabulary make an array with 1 * D for the word\n","    #   one_list = [0] * D\n","    #   tweet_vectors = one_list + prev_vector\n","    #   prev_vector = tweet_vectors\n","\n","  all_vectors_test.append(tweet_vectors/n_words) # divide with the sum of all elements to normalize values\n","\n","all_vectors_test = np.array(all_vectors_test)\n","\n","len(all_vectors_test)  # should be equal with the number of tweets from test set"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1639781126054,"user":{"displayName":"Nick","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09933765762641469766"},"user_tz":-120},"id":"AxLZB7cBSICI","outputId":"00efe4d6-b273-4e91-aa31-1b09c5a13bba"},"outputs":[{"output_type":"stream","name":"stdout","text":["x_train shape: torch.Size([15976, 25])\n","y_train shape: torch.Size([15976])\n"]}],"source":["# # prepare train set\n","import torch\n","\n","\n","\n","\n","x_train = (\n","    list(\n","        map(\n","            lambda x: torch.FloatTensor(x), \n","            all_vectors_train\n","        )\n","    )\n",")\n","\n","# Saving in tensors\n","x_train= torch.stack(x_train)\n","y_train = torch.tensor(df_train.label, dtype=torch.long)\n","\n","\n","print(f\"x_train shape: {x_train.shape}\")\n","print(f\"y_train shape: {y_train.shape}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P3aUpeHPWgkG","executionInfo":{"status":"ok","timestamp":1639781126576,"user_tz":-120,"elapsed":523,"user":{"displayName":"Nick","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09933765762641469766"}},"outputId":"0bae955d-8fb5-48bc-d426-2e6e97f61b76"},"outputs":[{"output_type":"stream","name":"stdout","text":["x_test shape: torch.Size([2282, 25])\n","y_test shape: torch.Size([2282])\n"]}],"source":["# # prepare test set\n","\n","x_test = (\n","    list(\n","        map(\n","            lambda x: torch.FloatTensor(x), \n","            all_vectors_test \n","        )\n","    )\n",")\n","\n","# Save in tensors\n","y_test = torch.tensor(df_test.label, dtype=torch.long)\n","\n","x_test= torch.stack(x_test)\n","\n","\n","print(f\"x_test shape: {x_test.shape}\")\n","print(f\"y_test shape: {y_test.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"zLAGm6-2inS9"},"source":["# Creating data loaders"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1FIyuyujwAy2"},"outputs":[],"source":["# Build data loaders\n","BatchSize = 1000\n","\n","# Initialize train dataloader\n","dataset = torch.utils.data.TensorDataset(x_train, y_train)\n","train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=BatchSize, shuffle=True)\n","\n","# Initialize test dataloader\n","dataset = torch.utils.data.TensorDataset(x_test, y_test)\n","test_dataloader = torch.utils.data.DataLoader(dataset, batch_size=BatchSize, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":273,"status":"ok","timestamp":1639782775972,"user":{"displayName":"Nick","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09933765762641469766"},"user_tz":-120},"id":"lniy3OQGernH","outputId":"18314f34-e0e6-42ec-de71-1705c1cb5600"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1000, 25]) torch.Size([1000])\n"]}],"source":["examples = iter(test_dataloader)\n","samples, labels = examples.next()\n","print(samples.shape, labels.shape)"]},{"cell_type":"markdown","metadata":{"id":"UoIz17OB0nnb"},"source":["# Creating Neural Networks"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn"],"metadata":{"id":"DF_LranzfA1z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U8dSDBk94-_O"},"source":["## Neural Network 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hw6L7XX51E0K"},"outputs":[],"source":["\n","class Network_1(nn.Module):\n","    def __init__(self, input_size, hidden_size1, hidden_size2, num_classes):\n","      super().__init__()\n","      self.linear1 = nn.Linear(input_size, hidden_size1)  \n","      self.relu1 = nn.ReLU()\n","      self.linear2 = nn.Linear(hidden_size1, hidden_size2)\n","      self.relu2 = nn.ReLU()\n","      self.linear3 = nn.Linear(hidden_size2, num_classes)\n","\n","\n","    def forward(self, x):\n","      out = self.linear1(x)\n","      out = self.relu1(out)\n","      out = self.linear2(out)\n","      out = self.relu2(out)\n","      out = self.linear3(out)\n","      return out \n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":280,"status":"ok","timestamp":1639783019752,"user":{"displayName":"Nick","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09933765762641469766"},"user_tz":-120},"id":"SVjQk_c44N-k","outputId":"ec38cd2c-9090-4648-cecd-7063d2832436"},"outputs":[{"output_type":"stream","name":"stdout","text":["weights: [0.71404309 2.56890175 0.8262736 ]\n"]}],"source":["# Creating the weights for the loss function \n","import sklearn.utils.class_weight as class_weight\n","\n","class_weights = class_weight.compute_class_weight(class_weight = 'balanced', classes = np.unique(y_train),y = y_train.numpy())\n","print('weights:', class_weights)\n","class_weights = torch.tensor(class_weights, dtype=torch.float)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":352,"status":"ok","timestamp":1639786276320,"user":{"displayName":"Nick","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09933765762641469766"},"user_tz":-120},"id":"Kzdvy57S3jtO","outputId":"2aa4ebd2-5947-4eb0-860f-dad715e1cfb4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Network_1(\n","  (linear1): Linear(in_features=25, out_features=400, bias=True)\n","  (relu1): ReLU()\n","  (linear2): Linear(in_features=400, out_features=300, bias=True)\n","  (relu2): ReLU()\n","  (linear3): Linear(in_features=300, out_features=3, bias=True)\n",")"]},"metadata":{},"execution_count":137}],"source":["#Define layer sizes\n","input_size = x_train.shape[1]\n","hidden_size1 = 400\n","hidden_size2 = 300\n","num_classes = 3\n","\n","#Define Hyperparameters\n","learning_rate = 1e-2\n","\n","#Initialize model, loss, optimizer\n","model_1 = Network_1(input_size, hidden_size1, hidden_size2, num_classes)\n","# loss_func = nn.CrossEntropyLoss()\n","loss_func = nn.CrossEntropyLoss(weight=class_weights, reduction='mean')\n","\n","\n","optimizer = torch.optim.Adamax(model_1.parameters(), lr=learning_rate)\n","\n","model_1"]},{"cell_type":"markdown","metadata":{"id":"On5WcmzyoR0P"},"source":["### Training the Neural Network 1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2997,"status":"ok","timestamp":1639786281067,"user":{"displayName":"Nick","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09933765762641469766"},"user_tz":-120},"outputId":"daf27663-5662-4c8e-c822-198957a2a9e2","id":"zSagHpYP-Su6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5: Train Loss = 1.2027  Test Loss = 1.0737  f1 score: 46.57%  precision score: 52.36%  recall score: 41.94%\n","Epoch 2/5: Train Loss = 1.0559  Test Loss = 1.0330  f1 score: 53.45%  precision score: 53.27%  recall score: 53.64%\n","Epoch 3/5: Train Loss = 1.0238  Test Loss = 1.0044  f1 score: 52.83%  precision score: 55.56%  recall score: 50.35%\n","Epoch 4/5: Train Loss = 1.0037  Test Loss = 0.9813  f1 score: 54.38%  precision score: 57.29%  recall score: 51.75%\n","Epoch 5/5: Train Loss = 0.9858  Test Loss = 0.9802  f1 score: 55.52%  precision score: 58.27%  recall score: 53.02%\n","\n","Average accuracy: 50.17 %\n","Average f1 score: 52.55 %\n","Average recall score: 50.14 %\n","Average precision score: 55.35 %\n"]}],"source":["from sklearn.metrics import f1_score, precision_score, recall_score\n","\n","num_epochs = 5\n","accuracies = []\n","average_f1_score = []\n","average_recall_score = []\n","average_precision_score = []\n","\n","for epoch in range(num_epochs):\n","  train_losses = []\n","  test_losses = []\n","\n","  \n","\n","  for x_batch, y_batch in train_dataloader: # gia ato train set, to idio gia to validation sto project\n","    y_pred = model_1(x_batch)\n","\n","    loss = loss_func(y_pred, y_batch) \n","\n","    train_losses.append(loss.item())\n","    \n","    #Delete previously stored gradients\n","    optimizer.zero_grad()\n","    #Perform backpropagation starting from the loss calculated in this epoch\n","    loss.backward()\n","    #Update model's weights based on the gradients calculated during backprop\n","    optimizer.step()\n","\n","  print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {np.mean(train_losses):.4f}\", end = '  ')\n"," \n","  # Testing the model_GloVe\n","  # with torch.no_grad():\n","  for x_batch, y_batch in test_dataloader:\n","\n","    y_pred = model_1(x_batch)\n","    \n","    loss = loss_func(y_pred, y_batch) # convert to type long\n","\n","    ps = torch.exp(y_pred)\n","    top_p, top_class = ps.topk(1, dim=1)\n","    equals = top_class == y_batch.view(*top_class.shape)\n","    \n","    test_losses.append(loss.item())\n","\n","    accuracy = torch.mean(equals.float())\n","    accuracies.append(accuracy)           \n","\n","  print(f\"Test Loss = {np.mean(test_losses):.4f}\", end = '  ')   \n","\n","  y_pred = model_1(x_test)\n","\n","  # Getting precision, recall, f1 scores and accuracy.\n","  precision_score_temp = precision_score(y_test, torch.argmax(y_pred, dim=1), average='weighted')\n","  recall_score_temp = recall_score(y_test, torch.argmax(y_pred, dim=1), labels=[0, 1, 2], average='weighted')\n","  f1_score_temp = 2 * (precision_score_temp * recall_score_temp) / (precision_score_temp + recall_score_temp)\n","\n","  average_precision_score.append(precision_score_temp)\n","  average_f1_score.append(f1_score_temp)\n","  average_recall_score.append(recall_score_temp)\n","  print(\"f1 score: {}%\".format(round(f1_score_temp*100, 2)), end = '  ')\n","  print(\"precision score: {}%\".format(round(precision_score_temp*100, 2)), end = '  ')\n","  print(\"recall score: {}%\".format(round(recall_score_temp*100, 2)))\n","\n","print()\n","print(\"Average accuracy: {} %\".format(round(np.mean(accuracies)*100, 2)))\n","print(\"Average f1 score: {} %\".format(round(np.mean(average_f1_score)*100, 2)))\n","print(\"Average recall score: {} %\".format(round(np.mean(average_recall_score)*100, 2)))\n","print(\"Average precision score: {} %\".format(round(np.mean(average_precision_score)*100, 2)))"]},{"cell_type":"markdown","source":["## Neural Network 2\n"],"metadata":{"id":"oAp2J_dI3_at"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_1qUy-oA4P6j"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","\n","class Network_2(nn.Module):\n","    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, num_classes):\n","      super().__init__()\n","      self.linear1 = nn.Linear(input_size, hidden_size1)  \n","      self.celu = nn.CELU()\n","      self.linear2 = nn.Linear(hidden_size1, hidden_size2)\n","      self.gelu = nn.GELU()\n","      self.linear3 = nn.Linear(hidden_size2, hidden_size3)\n","      self.elu = nn.ELU()\n","      self.linear4 = nn.Linear(hidden_size3, num_classes)\n","\n","\n","    def forward(self, x):\n","      out = self.linear1(x)\n","      out = self.celu(out)\n","      out = self.linear2(out)\n","      out = self.gelu(out)\n","      out = self.linear3(out)\n","      out = self.elu(out)\n","      out = self.linear4(out)\n","      return out \n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":372,"status":"ok","timestamp":1639786310210,"user":{"displayName":"Nick","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09933765762641469766"},"user_tz":-120},"outputId":"3f33cebd-7d57-45e4-97c7-a19939eea4ca","id":"XY75knOO4P6p"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Network_2(\n","  (linear1): Linear(in_features=25, out_features=200, bias=True)\n","  (celu): CELU(alpha=1.0)\n","  (linear2): Linear(in_features=200, out_features=100, bias=True)\n","  (gelu): GELU()\n","  (linear3): Linear(in_features=100, out_features=150, bias=True)\n","  (elu): ELU(alpha=1.0)\n","  (linear4): Linear(in_features=150, out_features=3, bias=True)\n",")"]},"metadata":{},"execution_count":139}],"source":["#Define layer sizes\n","input_size = x_train.shape[1]\n","hidden_size1 = 200\n","hidden_size2 = 100\n","hidden_size3 = 150\n","num_classes = 3\n","\n","#Define Hyperparameters\n","learning_rate = 1e-2\n","\n","#Initialize model, loss, optimizer\n","model_2 = Network_2(input_size, hidden_size1, hidden_size2, hidden_size3, num_classes)\n","# loss_func = nn.CrossEntropyLoss()\n","loss_func = nn.CrossEntropyLoss(weight=class_weights, reduction='mean')     # same weights with model_1\n","\n","\n","optimizer = torch.optim.AdamW(model_2.parameters(), lr=learning_rate)\n","\n","model_2"]},{"cell_type":"markdown","metadata":{"id":"cGDf3G4C4P6q"},"source":["### Training the Network 2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1804,"status":"ok","timestamp":1639786314283,"user":{"displayName":"Nick","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09933765762641469766"},"user_tz":-120},"outputId":"b4c94586-d636-4fe3-86ea-9d5d4e888ab3","id":"0yYmHc7b4P6q"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5: Train Loss = 1.0910  Test Loss = 1.0310  f1 score: 44.67%  precision score: 52.19%  recall score: 39.04%\n","Epoch 2/5: Train Loss = 1.0245  Test Loss = 1.0000  f1 score: 52.43%  precision score: 57.85%  recall score: 47.94%\n","Epoch 3/5: Train Loss = 0.9919  Test Loss = 1.0052  f1 score: 48.85%  precision score: 57.17%  recall score: 42.64%\n","Epoch 4/5: Train Loss = 0.9845  Test Loss = 0.9686  f1 score: 53.87%  precision score: 57.3%  recall score: 50.83%\n","Epoch 5/5: Train Loss = 0.9733  Test Loss = 0.9656  f1 score: 53.7%  precision score: 57.59%  recall score: 50.31%\n","\n","Average accuracy: 46.28 %\n","Average f1 score: 50.7 %\n","Average recall score: 46.15 %\n","Average precision score: 56.42 %\n"]}],"source":["from sklearn.metrics import f1_score, precision_score, recall_score\n","\n","num_epochs = 5\n","accuracies = []\n","average_f1_score = []\n","average_recall_score = []\n","average_precision_score = []\n","\n","\n","for epoch in range(num_epochs):\n","  train_losses = []\n","  test_losses = []\n","  \n","\n","  for x_batch, y_batch in train_dataloader: # gia ato train set, to idio gia to validation sto project\n","    y_pred = model_2(x_batch)\n","\n","    loss = loss_func(y_pred, y_batch) \n","\n","    train_losses.append(loss.item())\n","    \n","    #Delete previously stored gradients\n","    optimizer.zero_grad()\n","    #Perform backpropagation starting from the loss calculated in this epoch\n","    loss.backward()\n","    #Update model's weights based on the gradients calculated during backprop\n","    optimizer.step()\n","\n","  print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {np.mean(train_losses):.4f}\", end = '  ')\n"," \n","  # for test loop\n","  # with torch.no_grad():\n","  for x_batch, y_batch in test_dataloader:\n","\n","    y_pred = model_2(x_batch)\n","    \n","    loss = loss_func(y_pred, y_batch) # convert to type long\n","\n","    ps = torch.exp(y_pred)\n","    top_p, top_class = ps.topk(1, dim=1)\n","    equals = top_class == y_batch.view(*top_class.shape)\n","    \n","    test_losses.append(loss.item())\n","\n","\n","    accuracy = torch.mean(equals.float())\n","    accuracies.append(accuracy)\n","\n","  print(f\"Test Loss = {np.mean(test_losses):.4f}\", end = '  ')\n","\n","  y_pred = model_2(x_test)\n","\n","  # Getting precision, recall, f1 scores and accuracy.\n","  precision_score_temp = precision_score(y_test, torch.argmax(y_pred, dim=1), average='weighted')\n","  recall_score_temp = recall_score(y_test, torch.argmax(y_pred, dim=1), labels=[0, 1, 2], average='weighted')\n","  f1_score_temp = 2 * (precision_score_temp * recall_score_temp) / (precision_score_temp + recall_score_temp)\n","\n","  average_precision_score.append(precision_score_temp)\n","  average_f1_score.append(f1_score_temp)\n","  average_recall_score.append(recall_score_temp)\n","  print(\"f1 score: {}%\".format(round(f1_score_temp*100, 2)), end = '  ')\n","  print(\"precision score: {}%\".format(round(precision_score_temp*100, 2)), end = '  ')\n","  print(\"recall score: {}%\".format(round(recall_score_temp*100, 2)))\n","\n","print()\n","print(\"Average accuracy: {} %\".format(round(np.mean(accuracies)*100, 2)))\n","print(\"Average f1 score: {} %\".format(round(np.mean(average_f1_score)*100, 2)))\n","print(\"Average recall score: {} %\".format(round(np.mean(average_recall_score)*100, 2)))\n","print(\"Average precision score: {} %\".format(round(np.mean(average_precision_score)*100, 2)))"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"project2_GloVe.ipynb","toc_visible":true,"provenance":[],"authorship_tag":"ABX9TyNLhGRc5MlXEKQXh3dXxi5v"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}